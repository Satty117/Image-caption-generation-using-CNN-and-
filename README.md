This project focuses on advancing image captioning proficiency by developing a robust algorithm that leverages Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), specifically Long Short-Term Memory (LSTM) networks, to generate descriptive captions for images. This initiative addresses the growing demand for an efficient method of interpreting visual content across various domains, including accessibility, search and indexing, biomedicine, e-commerce, social media, content moderation, and visual storytelling. By harnessing the power of CNN for feature extraction and RNN-LSTM for sequential description generation, it aims to enhance the accuracy and contextuality of image captions. Leveraging advancements in computer vision, natural language processing (NLP), and machine learning, it aims to advance the technology of image captioning for practical use in real-world scenarios. In addition to traditional datasets, this approach involves real-time image capture through OpenCV, accessing the camera system. By integrating real-world environments, it aims to expand the diversity and applicability of datasets used to train and evaluate captioning models. Through the integration of live image capture, this project seeks to enhance the practicality of image-captioning techniques, enabling real-time captioning across different devices and platforms.
